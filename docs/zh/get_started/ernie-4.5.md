[English](../../get_started/ernie-4.5.md)

# ERNIE-4.5æ¨¡å‹

æœ¬æ–‡æ¡£è®²è§£å¦‚ä½•éƒ¨ç½²ERNIE-4.5æ¨¡å‹ï¼Œåœ¨å¼€å§‹éƒ¨ç½²å‰ï¼Œè¯·ç¡®ä¿ä½ çš„ç¡¬ä»¶ç¯å¢ƒæ»¡è¶³å¦‚ä¸‹æ¡ä»¶ï¼š

- GPUé©±åŠ¨ >= 535
- CUDA >= 12.3
- CUDNN >= 9.5
- Linux X86_64
- Python >= 3.10
- 80G A/H 4å¡

å®‰è£…FastDeployæ–¹å¼å‚è€ƒ[å®‰è£…æ–‡æ¡£](./installation/README.md)ã€‚

## å‡†å¤‡æ¨¡å‹
éƒ¨ç½²æ—¶æŒ‡å®š ```--model baidu/ERNIE-4.5-300B-A47B-Paddle``` å³å¯è‡ªåŠ¨ä»AIStudioä¸‹è½½æ¨¡å‹ï¼Œå¹¶æ”¯æŒæ–­ç‚¹ç»­ä¼ ã€‚ä½ ä¹Ÿå¯ä»¥è‡ªè¡Œä»ä¸åŒæ¸ é“ä¸‹è½½æ¨¡å‹ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯FastDeployä¾èµ–Paddleæ ¼å¼çš„æ¨¡å‹ï¼Œæ›´å¤šè¯´æ˜å‚è€ƒ[æ”¯æŒæ¨¡å‹åˆ—è¡¨](../supported_models.md)ã€‚

## å¯åŠ¨æœåŠ¡

>ğŸ’¡ **æ³¨æ„**ï¼š ç”±äºæ¨¡å‹å‚æ•°é‡ä¸º300B-A47Bï¼Œåœ¨80G * 8å¡çš„æœºå™¨ä¸Šï¼Œéœ€æŒ‡å®š ```--quantization wint4``` (wint8ä¹Ÿå¯éƒ¨ç½²ï¼Œå…¶ä¸­wint4 4å¡å³å¯éƒ¨ç½²ï¼Œwint8åˆ™éœ€è¦8å¡)ã€‚

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¯åŠ¨æœåŠ¡ï¼Œå…¶ä¸­å¯åŠ¨å‘½ä»¤é…ç½®æ–¹å¼å‚è€ƒ[å‚æ•°è¯´æ˜](../parameters.md)ã€‚

```shell
export ENABLE_V1_KVCACHE_SCHEDULER=1
python -m fastdeploy.entrypoints.openai.api_server \
       --model baidu/ERNIE-4.5-300B-A47B-Paddle \
       --port 8180 --engine-worker-queue-port 8181 \
       --cache-queue-port 8183 --metrics-port 8182 \
       --tensor-parallel-size 8 \
       --quantization wint4 \
       --max-model-len 32768 \
       --max-num-seqs 32
```

## ç”¨æˆ·å‘èµ·æœåŠ¡è¯·æ±‚
æ‰§è¡Œå¯åŠ¨æœåŠ¡æŒ‡ä»¤åï¼Œå½“ç»ˆç«¯æ‰“å°å¦‚ä¸‹ä¿¡æ¯ï¼Œè¯´æ˜æœåŠ¡å·²ç»å¯åŠ¨æˆåŠŸã€‚

```shell
api_server.py[line:91] Launching metrics service at http://0.0.0.0:8181/metrics
api_server.py[line:94] Launching chat completion service at http://0.0.0.0:8180/v1/chat/completions
api_server.py[line:97] Launching completion service at http://0.0.0.0:8180/v1/completions
INFO:     Started server process [13909]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8180 (Press CTRL+C to quit)
```

FastDeployæä¾›æœåŠ¡æ¢æ´»æ¥å£ï¼Œç”¨ä»¥åˆ¤æ–­æœåŠ¡çš„å¯åŠ¨çŠ¶æ€ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤è¿”å› ```HTTP/1.1 200 OK``` å³è¡¨ç¤ºæœåŠ¡å¯åŠ¨æˆåŠŸã€‚

```shell
curl -i http://0.0.0.0:8180/health
```

é€šè¿‡å¦‚ä¸‹å‘½ä»¤è¿›è¡ŒæœåŠ¡è¯·æ±‚ã€‚

```shell
curl -X POST "http://0.0.0.0:8180/v1/chat/completions" \
-H "Content-Type: application/json" \
-d '{
  "messages": [
    {"role": "user", "content": "æŠŠæç™½çš„é™å¤œæ€æ”¹å†™ä¸ºç°ä»£è¯—"}
  ]
}'
```

FastDeployæœåŠ¡æ¥å£å…¼å®¹OpenAIåè®®ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹Pythonä»£ç å‘èµ·æœåŠ¡è¯·æ±‚ã€‚

```python
import openai
host = "0.0.0.0"
port = "8180"
client = openai.Client(base_url=f"http://{host}:{port}/v1", api_key="null")

response = client.chat.completions.create(
    model="null",
    messages=[
        {"role": "system", "content": "I'm a helpful AI assistant."},
        {"role": "user", "content": "æŠŠæç™½çš„é™å¤œæ€æ”¹å†™ä¸ºç°ä»£è¯—"},
    ],
    stream=True,
)
for chunk in response:
    if chunk.choices[0].delta:
        print(chunk.choices[0].delta.content, end='')
print('\n')
```
