[English](../../get_started/quick_start_qwen.md)

# 10åˆ†é’Ÿå®Œæˆ Qwen3-0.6b æ¨¡å‹éƒ¨ç½²

æœ¬æ–‡æ¡£è®²è§£å¦‚ä½•éƒ¨ç½²Qwen3-0.6bæ¨¡å‹ï¼Œåœ¨å¼€å§‹éƒ¨ç½²å‰ï¼Œè¯·ç¡®ä¿ä½ çš„ç¡¬ä»¶ç¯å¢ƒæ»¡è¶³å¦‚ä¸‹æ¡ä»¶ï¼š

- GPUé©±åŠ¨ >= 535
- CUDA >= 12.3
- CUDNN >= 9.5
- Linux X86_64
- Python >= 3.10

ä¸ºäº†å¿«é€Ÿåœ¨å„ç±»ç¡¬ä»¶éƒ¨ç½²ï¼Œæœ¬æ–‡æ¡£é‡‡ç”¨ ```Qwen3-0.6b``` æ¨¡å‹ä½œä¸ºç¤ºä¾‹ï¼Œå¯åœ¨å¤§éƒ¨åˆ†ç¡¬ä»¶ä¸Šå®Œæˆéƒ¨ç½²ã€‚

å®‰è£…FastDeployæ–¹å¼å‚è€ƒ[å®‰è£…æ–‡æ¡£](./installation/README.md)ã€‚
## 1. å¯åŠ¨æœåŠ¡
å®‰è£…FastDeployåï¼Œåœ¨ç»ˆç«¯æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¯åŠ¨æœåŠ¡ï¼Œå…¶ä¸­å¯åŠ¨å‘½ä»¤é…ç½®æ–¹å¼å‚è€ƒ[å‚æ•°è¯´æ˜](../parameters.md)

```shell
export ENABLE_V1_KVCACHE_SCHEDULER=1
python -m fastdeploy.entrypoints.openai.api_server \
       --model Qwen/Qwen3-0.6B\
       --port 8180 \
       --metrics-port 8181 \
       --engine-worker-queue-port 8182 \
       --max-model-len 32768 \
       --max-num-seqs 32
```

>ğŸ’¡ æ³¨æ„ï¼šåœ¨ ```--model``` æŒ‡å®šçš„è·¯å¾„ä¸­ï¼Œè‹¥å½“å‰ç›®å½•ä¸‹ä¸å­˜åœ¨è¯¥è·¯å¾„å¯¹åº”çš„å­ç›®å½•ï¼Œåˆ™ä¼šå°è¯•æ ¹æ®æŒ‡å®šçš„æ¨¡å‹åç§°ï¼ˆå¦‚ ```Qwen/Qwen3-0.6B```ï¼‰æŸ¥è¯¢AIStudioæ˜¯å¦å­˜åœ¨é¢„ç½®æ¨¡å‹ï¼Œè‹¥å­˜åœ¨ï¼Œåˆ™è‡ªåŠ¨å¯åŠ¨ä¸‹è½½ã€‚é»˜è®¤çš„ä¸‹è½½è·¯å¾„ä¸ºï¼š```~/xx```ã€‚å…³äºæ¨¡å‹è‡ªåŠ¨ä¸‹è½½çš„è¯´æ˜å’Œé…ç½®å‚é˜…[æ¨¡å‹ä¸‹è½½](../supported_models.md)ã€‚
```--max-model-len``` è¡¨ç¤ºå½“å‰éƒ¨ç½²çš„æœåŠ¡æ‰€æ”¯æŒçš„æœ€é•¿Tokenæ•°é‡ã€‚
```--max-num-seqs``` è¡¨ç¤ºå½“å‰éƒ¨ç½²çš„æœåŠ¡æ‰€æ”¯æŒçš„æœ€å¤§å¹¶å‘å¤„ç†æ•°é‡ã€‚

**ç›¸å…³æ–‡æ¡£**

- [æœåŠ¡éƒ¨ç½²é…ç½®](../online_serving/README.md)
- [æœåŠ¡ç›‘æ§metrics](../online_serving/metrics.md)

## 2. ç”¨æˆ·å‘èµ·æœåŠ¡è¯·æ±‚

æ‰§è¡Œå¯åŠ¨æœåŠ¡æŒ‡ä»¤åï¼Œå½“ç»ˆç«¯æ‰“å°å¦‚ä¸‹ä¿¡æ¯ï¼Œè¯´æ˜æœåŠ¡å·²ç»å¯åŠ¨æˆåŠŸã€‚

```
api_server.py[line:91] Launching metrics service at http://0.0.0.0:8181/metrics
api_server.py[line:94] Launching chat completion service at http://0.0.0.0:8180/v1/chat/completions
api_server.py[line:97] Launching completion service at http://0.0.0.0:8180/v1/completions
INFO:     Started server process [13909]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8180 (Press CTRL+C to quit)
```

FastDeployæä¾›æœåŠ¡æ¢æ´»æ¥å£ï¼Œç”¨ä»¥åˆ¤æ–­æœåŠ¡çš„å¯åŠ¨çŠ¶æ€ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤è¿”å› ```HTTP/1.1 200 OK``` å³è¡¨ç¤ºæœåŠ¡å¯åŠ¨æˆåŠŸã€‚

```shell
curl -i http://0.0.0.0:8180/health
```

é€šè¿‡å¦‚ä¸‹å‘½ä»¤å‘èµ·æœåŠ¡è¯·æ±‚

```shell
curl -X POST "http://0.0.0.0:8180/v1/chat/completions" \
-H "Content-Type: application/json" \
-d '{
  "messages": [
    {"role": "user", "content": "æŠŠæç™½çš„é™å¤œæ€æ”¹å†™ä¸ºç°ä»£è¯—"}
  ]
}'
```

FastDeployæœåŠ¡æ¥å£å…¼å®¹OpenAIåè®®ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹Pythonä»£ç å‘èµ·æœåŠ¡è¯·æ±‚ã€‚

```python
import openai
host = "0.0.0.0"
port = "8180"
client = openai.Client(base_url=f"http://{host}:{port}/v1", api_key="null")

response = client.chat.completions.create(
    model="null",
    messages=[
        {"role": "system", "content": "I'm a helpful AI assistant."},
        {"role": "user", "content": "æŠŠæç™½çš„é™å¤œæ€æ”¹å†™ä¸ºç°ä»£è¯—"},
    ],
    stream=True,
)
for chunk in response:
    if chunk.choices[0].delta:
        print(chunk.choices[0].delta.content, end='')
print('\n')
```
