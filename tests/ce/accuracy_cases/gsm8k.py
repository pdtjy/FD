#!/bin/env python3
# -*- coding: utf-8 -*-
# @author DDDivano
# encoding=utf-8 vi:ts=4:sw=4:expandtab:ft=python


import os
import re
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse, urlunparse

import openai
from datasets import load_dataset
from tqdm import tqdm

BASELINE = {
    "0.3B": 0.05,
    "21B": 0.49,
    "300B": 0.96,
}
baseline = BASELINE.get(os.environ.get("MODEL_SIZE"), None)
base_url = os.environ.get("URL", None)
atol = 0.03
if baseline is None:
    raise ValueError(
        f"Invalid MODEL_SIZE value '{os.environ.get('MODEL_SIZE')}', expected one of {list(BASELINE.keys())}"
    )
if base_url is None:
    raise ValueError(
        "Environment variable 'URL' is not set. "
        "Please specify the inference service address, e.g., 'http://localhost:8191/v1'."
    )


def strip_path_suffix(url: str, suffix: str = "chat/completions") -> str:
    """
    å»é™¤ URL ä¸­çš„æŒ‡å®šè·¯å¾„åç¼€ï¼ˆå¦‚ chat/completionsï¼‰
    """
    parsed = urlparse(url)
    # ç§»é™¤æœ«å°¾çš„ suffixï¼ˆæ³¨æ„ç¡®ä¿åªç§»é™¤ç»“å°¾éƒ¨åˆ†ï¼‰
    if parsed.path.endswith("/" + suffix):
        new_path = parsed.path[: -(len(suffix) + 1)]  # +1 æ˜¯æ–œæ 
    else:
        new_path = parsed.path
    # é‡æ–°æ„é€  URL
    cleaned_url = urlunparse(
        (
            parsed.scheme,
            parsed.netloc,
            new_path.rstrip("/"),  # å»æ‰æœ«å°¾çš„æ–œæ 
            "",
            "",
            "",  # å¿½ç•¥ params/query/fragment
        )
    )
    return cleaned_url


# ========== OpenAI å®¢æˆ·ç«¯é…ç½® ==========
client = openai.OpenAI(
    api_key="DDDivano",
    # base_url="http://å ä½:8187/v1"
    base_url=strip_path_suffix(base_url),
)

model_name = "eb"
max_samples = 690
max_tokens = 12288
max_workers = 33

# ========== åŠ è½½æ•°æ®é›† ==========
dataset = load_dataset("parquet", data_files="gsm8k.parquet", split="train")
dataset = dataset.select(range(min(len(dataset), max_samples)))


# ========== æå– GT ä¸­ "#### æ•°å­—" æ ¼å¼çš„æœ€ç»ˆç­”æ¡ˆ ==========
def extract_gt_answer(text):
    match = re.search(r"####\s*([\d,]+(?:\.\d+)?)", text)
    if match:
        return match.group(1).replace(",", "").strip()
    return None


# ========== æå–æ¨¡å‹è¾“å‡ºä¸­çš„â€œæœ€åä¸€å¥è¯â€ä¸­çš„æ•°å­— ==========
def extract_model_answer(text):
    if not text:
        return None
    text = text.replace(",", "").replace("$", "")
    lines = text.strip().splitlines()
    last_line = lines[-1] if lines else text
    match = re.search(r"-?\d+(?:\.\d+)?", last_line)
    return match.group(0) if match else None


# ========== æ•°å€¼æ¯”è¾ƒå‡½æ•° ==========
def is_answer_equal(pred, gt, tol=1e-6):
    if pred is None or gt is None:
        return False
    try:
        return abs(float(pred) - float(gt)) < tol
    except:
        return pred == gt


# ========== æ„é€  Prompt ==========
def build_prompt(sample):
    return f"ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ•°å­¦é—®é¢˜ï¼Œè¯·ç›´æ¥ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚ä¸€å®šè¦æŠŠæœ€ç»ˆç­”æ¡ˆæ•°å­—åœ¨æœ€åè¾“å‡ºã€‚\n\né—®é¢˜ï¼š{sample['question']}\n\nç­”æ¡ˆï¼š"


# ========== æ¨¡å‹è¯·æ±‚å‡½æ•° ==========
def query_model(prompt):
    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶ï¼Œæ“…é•¿ä¸¥è°¨åœ°è§£ç­”æ•°å­¦é—®é¢˜ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=1.0,
            top_p=0.8,
            max_tokens=max_tokens,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[Error] {e}, {str(traceback.format_exc())}"


# ========== è¯„ä¼°å‡½æ•° ==========
def evaluate_sample(sample):
    prompt = build_prompt(sample)
    model_output = query_model(prompt)

    gt_value = extract_gt_answer(sample["answer"])
    pred_value = extract_model_answer(model_output)
    is_correct = is_answer_equal(pred_value, gt_value)

    result = {
        "question": sample["question"],
        "gt_answer": gt_value,
        "model_answer": pred_value,
        "raw_gt_answer": sample["answer"],
        "raw_model_output": model_output,
        "is_correct": is_correct,
    }

    return result


# ========== ä¸»æµç¨‹ ==========

acc = []
times = 3

for i in range(times):
    correct = 0
    total = 0
    results = []

    print(f"ğŸš€ Starting evaluation with {max_workers} threads...")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(evaluate_sample, sample) for sample in dataset]
        for future in tqdm(as_completed(futures), total=len(futures), desc="Evaluating"):
            result = future.result()
            results.append(result)
            total += 1
            if result["is_correct"]:
                correct += 1
            else:
                print("\nâŒ Wrong prediction:")
                print(f"Q: {result['question']}")
                print(f"GT: {result['gt_answer']}")
                print(f"Model: {result['model_answer']}")
                print(f"Full GT: {result['raw_gt_answer']}")
                print(f"Model Output: {result['raw_model_output']}")

    # ========== è¾“å‡ºå‡†ç¡®ç‡ ==========
    accuracy = correct / total * 100 if total > 0 else 0.0
    print(f"\nğŸ¯ Evaluation Complete: Accuracy = {accuracy:.2f}% ({correct}/{total})")
    acc.append(accuracy)

avg_acc = round(sum(acc) / times / 100, 4)  # ä¼˜åŒ–ç™¾åˆ†æ•°
print(f"å¹³å‡å‡†ç¡®ç‡ï¼š{avg_acc * 100:.2f}%")

assert (
    abs(avg_acc - baseline) <= atol
), f"æ¨¡å‹å‡†ç¡®ç‡ {avg_acc:.2f} ä¸åŸºå‡† {baseline:.2f} ç›¸å·® {abs(avg_acc - baseline):.2f}ï¼Œè¶…å‡ºå®¹å¿èŒƒå›´ {atol:.2f}"

# with open("eval_result_math.json", "w", encoding="utf-8") as f:
#     json.dump(results, f, indent=2, ensure_ascii=False)
